{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic chatbot using Keras and NLTK\n",
        "\n",
        "## Introduction \n",
        "\n",
        "A chatbot is smart code that is capable of communicating similar to a human.\n",
        "\n",
        "Chatbots are used a lot in customer interaction, marketing on social network sites and instantly messaging the client.\n",
        "\n",
        "There are two basic types of Natural Language Processing (NLP) chatbot models based on how they are built:\n",
        "\n",
        "Retrieval based. A retrieval-based chatbot uses predefined input patterns and responses. It then uses some type of heuristic approach to select the appropriate response. It is widely used in the industry to make goal-oriented chatbots where we can customize the tone and flow of the chatbot to drive our customers with the best experience.\n",
        "\n",
        "Generative based models are not based on some predefined responses. They are based on seq 2 seq neural networks. It is the same idea as machine translation. In machine translation, we translate the source code from one language to another language but here, we are going to transform input into an output. It needs a large amount of data and it is based on Deep Neural Networks (DNN).\n",
        "\n"
      ],
      "metadata": {
        "id": "N-CcKpTQYXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#upload = files.upload()\n",
        "#!cp -r \"/content/Chatbot\" \"/content/drive/MyDrive\""
      ],
      "metadata": {
        "id": "j6yiGSp5C0pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries\n"
      ],
      "metadata": {
        "id": "ciJv_24AZJrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n3qt_JU-lhlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad94d1f-6d09-4cbe-a078-e8d2653d6f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pickle           #for serialisation\n",
        "import random\n",
        "import numpy as np \n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "nltk.download('punkt')  # dependencies required \n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "path_json = '/content/drive/MyDrive/Chatbot/intents.json'\n",
        "intents = json.loads(open(path_json).read())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the chatbot\n"
      ],
      "metadata": {
        "id": "t0iQByHpcOjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = {'?', '!', '.', ','}\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        word_list = nltk.word_tokenize(pattern)\n",
        "        words.extend(word_list)\n",
        "        documents.append((word_list, intent['tag']))\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "print(documents)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnH7KdNTbauF",
        "outputId": "515fc366-731d-43ba-ea6a-187c0a1be314"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['hello'], 'greeting'), (['hi'], 'greeting'), (['good', 'day'], 'greeting'), (['what', \"'s\", 'up', '?'], 'greeting'), (['how', \"'s\", 'it', 'going', '?'], 'greeting'), (['Greetings'], 'greeting'), (['hola'], 'greeting'), (['hey', 'there'], 'greeting'), (['Excuse', 'me'], 'greeting'), (['Anybody', 'there', '?'], 'greeting'), (['hola'], 'greeting'), (['cya'], 'goodbye'), (['See', 'you', 'later'], 'goodbye'), (['Goodbye'], 'goodbye'), (['I', 'am', 'Leaving'], 'goodbye'), (['Have', 'a', 'Good', 'day'], 'goodbye'), (['bye'], 'goodbye'), (['ciao'], 'goodbye'), (['see', 'ya'], 'goodbye'), (['how', 'are', 'you', '?'], 'greet'), (['How', \"'s\", 'it', 'going', '?'], 'greet'), (['Are', 'you', 'good', '?'], 'greet'), (['How', 'have', 'you', 'been', '?'], 'greet'), (['how', \"'s\", 'you', '?'], 'greet'), (['what', 'stocks', 'do', 'I', 'own', '?'], 'stocks'), (['how', 'are', 'my', 'shares', '?'], 'stocks'), (['what', 'companies', 'am', 'I', 'investing', 'in', '?'], 'stocks'), (['what', 'am', 'I', 'doing', 'in', 'the', 'markets', '?'], 'stocks'), (['how', 'old', 'are', 'you', '?'], 'age'), (['What', 'is', 'your', 'age', '?'], 'age'), (['age', '?'], 'age'), (['how', 'old', '?'], 'age'), (['what', 'is', 'your', 'name', '?'], 'name'), (['what', \"'s\", 'your', 'name', '?'], 'name'), (['name', '?'], 'name'), (['what', 'should', 'I', 'call', 'you', '?'], 'name'), (['who', 'are', 'you', '?'], 'name'), (['what', 'are', 'you', '?'], 'name')]\n",
            "['hello', 'hi', 'good', 'day', 'what', \"'s\", 'up', '?', 'how', \"'s\", 'it', 'going', '?', 'Greetings', 'hola', 'hey', 'there', 'Excuse', 'me', 'Anybody', 'there', '?', 'hola', 'cya', 'See', 'you', 'later', 'Goodbye', 'I', 'am', 'Leaving', 'Have', 'a', 'Good', 'day', 'bye', 'ciao', 'see', 'ya', 'how', 'are', 'you', '?', 'How', \"'s\", 'it', 'going', '?', 'Are', 'you', 'good', '?', 'How', 'have', 'you', 'been', '?', 'how', \"'s\", 'you', '?', 'what', 'stocks', 'do', 'I', 'own', '?', 'how', 'are', 'my', 'shares', '?', 'what', 'companies', 'am', 'I', 'investing', 'in', '?', 'what', 'am', 'I', 'doing', 'in', 'the', 'markets', '?', 'how', 'old', 'are', 'you', '?', 'What', 'is', 'your', 'age', '?', 'age', '?', 'how', 'old', '?', 'what', 'is', 'your', 'name', '?', 'what', \"'s\", 'your', 'name', '?', 'name', '?', 'what', 'should', 'I', 'call', 'you', '?', 'who', 'are', 'you', '?', 'what', 'are', 'you', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb')) #savin the lists in a pickle file in the form of binaries\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "\n",
        "#creating the bow list and the training list\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "    bow = []\n",
        "    word_patterns = document[0]\n",
        "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "    for word in words:\n",
        "        bow.append(1) if word in word_patterns else bow.append(0)\n",
        "        \n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(document[1])] = 1 \n",
        "    training.append([bow, output_row])\n",
        "\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "\n",
        "#building and training the neural network\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape = (len(train_x[0]),), activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation = 'softmax'))\n",
        "\n",
        "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov= True)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs = 500, batch_size = 5, verbose =1)\n",
        "model.save('chatbotmodel.h5', hist)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "rTvQSMR-b_FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing and Function Definition"
      ],
      "metadata": {
        "id": "u5xSa-Mtaa-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = pickle.load(open('words.pkl', 'rb'))\n",
        "classes = pickle.load(open('classes.pkl', 'rb'))\n",
        "model = load_model('chatbotmodel.h5')\n",
        "\n",
        "def  clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "\n",
        "def  bag_of_words(sentence):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bow = [0]*len(words)\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bow[i] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "\n",
        "def predict_class(sentence):\n",
        "  bag = bag_of_words(sentence)\n",
        "  res = model.predict(np.array([bag]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  results = [[i,r] for i,r in enumerate(res) if r > ERROR_THRESHOLD]\n",
        "\n",
        "  results.sort(key = lambda x: x[1], reverse = True)\n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "  return return_list \n",
        "\n",
        "def get_response(intents_list, intents_json):\n",
        "  tag = intents_list[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if i['tag'] == tag:\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result"
      ],
      "metadata": {
        "id": "mDooP5qP1Ajl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the Chatbot"
      ],
      "metadata": {
        "id": "AYhVJQfgd4v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Speak to me..\")\n",
        "\n",
        "while True:\n",
        "  message = input(\"\")\n",
        "  ints = predict_class(message)\n",
        "  res = get_response(ints, intents)\n",
        "  print(res)\n",
        "  if ints[0]['intent'] == 'goodbye':\n",
        "    break\n"
      ],
      "metadata": {
        "id": "tJc_d0-dNjiy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c2004a-8bc8-4673-f66c-333654b860dd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speak to me..\n",
            "hello\n",
            "Hey!\n",
            "how are you\n",
            "Not too bad! Wbu?\n",
            "bye\n",
            "Tata!\n"
          ]
        }
      ]
    }
  ]
}